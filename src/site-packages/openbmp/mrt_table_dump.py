"""OpenBMP MRT

  Copyright (c) 2013-2016 Cisco Systems, Inc. and others.  All rights reserved.
  This program and the accompanying materials are made available under the
  terms of the Eclipse Public License v1.0 which accompanies this distribution,
  and is available at http://www.eclipse.org/legal/epl-v10.html

  .. moduleauthor:: Tim Evens <tievens@cisco.com>
"""
import multiprocessing
import time
import threading
import os
import socket
import openbmp.bgp as bgp

from struct import pack
from openbmp.bmp import resolveIp
from openbmp.db_mysql import DbMySQL
from openbmp.logger import init_mp_logger

#: MRT types and subtypes
TYPE_TABLE_DUMP_V2 = 13
SUBTYPE_DUMP_V2_PEER_INDEX = 1
SUBTYPE_DUMP_V2_RIB_IPV4_UNICAST = 2
SUBTYPE_DUMP_V2_RIB_IPV6_UNICAST = 4

class MrtTableDumper(multiprocessing.Process):
    """ OpenBMP DB_REST API rib dump to MRT TABLE_DUMP

        Runs in a loop to generate RIB dumps based on table dump configuration.
    """
    def __init__(self, cfg, log_queue):
        """ Constructor

            :param cfg:               Configuration dictionary
            :param log_queue:         Logging queue - sync logging
        """
        multiprocessing.Process.__init__(self)
        self._stop = multiprocessing.Event()

        self._thr_list = {}
        self._db_conn = None
        self._cfg = cfg
        self._log_queue = log_queue
        self.LOG = None

        self._interval = int(cfg['table_dump']['interval']['minutes'])
        if self._interval < 15:
            self._interval = 900    # to seconds
        else:
            self._interval *= 60    # to seconds

    def run(self):
        """ Override """
        self.LOG = init_mp_logger("mrt_table_dumper", self._log_queue)

        self.LOG.info("Running mrt_table_dumper")

        try:
            # wait for the mapping config to be loaded
            while not self.stopped():
                if self._cfg and 'logging' in self._cfg:
                    break

            # A table dump interval might be large and waiting till the next interval might not be desirable
            current_time = time.time()
            next_dump_time = self.get_next_dump_time(current_time)
            if self._cfg['table_dump']['dump_when_started'] == True and (next_dump_time - current_time) > 300:

                self.LOG.info("Next interval is greater than 5 minutes. Running dump now since dumper just started")
                self.do_dump(time.time())

            # Loop till stopped
            while not self.stopped():
                current_time = time.time()
                next_dump_time = self.get_next_dump_time(current_time)

                if (next_dump_time - current_time) > 0:
                    time.sleep(next_dump_time - current_time)

                self.do_dump(next_dump_time)
                time.sleep(0.200)

        except KeyboardInterrupt:
            pass

        self.LOG.info("mrt_table_dumper stopped")

    def get_next_dump_time(self, current_time):
        """ Get the next dump time in seconds based on current time

            :param current_time:        Current time.time()
            :return: Returns the time in seconds when the dump should be run next
        """
        return ((long(current_time) / self._interval) * self._interval) + self._interval

    def do_dump(self, dump_timestamp):
        """ Perform the TABLE_DUMP for all routers/peers configured

            :param dump_timestamp:      timestamp in seconds used for the log filename
        """
        (tm_year, tm_mon, tm_mday, tm_hour,
         tm_min, tm_sec, tm_wday, tm_yday, tm_isdst) = time.gmtime(dump_timestamp)

        file_date = "%02d-%02d-%02d_%02d-%02d" % (tm_year, tm_mon, tm_mday, tm_hour, tm_min)

        self.LOG.info("Performing table dump for time %02d-%02d-%02d %02d:%02d:%02d",
                      tm_year, tm_mon, tm_mday, tm_hour, tm_min, tm_sec)

        self._db_conn = DbMySQL(self.LOG, self._cfg['table_dump']['mysql']['username'],
                                self._cfg['table_dump']['mysql']['password'],
                                self._cfg['table_dump']['mysql']['host'],
                                self._cfg['table_dump']['mysql']['database'])

        #
        # QUERY for PEERS - sorted by collector, router
        #
        peers = self._db_conn.query(
                "select c.admin_id, r.ip_address,"
                "            p.peer_addr,"
                "            if(p.isIPv4, 'IPV4', 'IPV6'),p.hash_id,"
                "            p.peer_bgp_id,p.peer_as "
                "   from bgp_peers p join routers r on (p.router_hash_id = r.hash_id)"
                "        join collectors c on (r.collector_hash_id = c.hash_id)"
                "   order by c.admin_id,r.hash_id,p.peer_addr")

        if peers:
            self.LOG.info("Dump includes %d peers", len(peers))
            #
            # Run a table dump for each peer
            #
            for peer in peers:
                filename = os.path.join(self._cfg['table_dump']['log_dir'],
                                        'COLLECTOR_' + str(peer[0]),
                                        'ROUTER_' + resolveIp(str(peer[1])),
                                        'PEER_' + str(peer[3]) + '_' + resolveIp(str(peer[2])),
                                        'table.mrt.' + file_date)
                filename = str(filename)

                self.LOG.debug("%s %s %s file=%s" % (peer[0], peer[1], peer[2], filename))

                # Make sure parent dirs exist
                if not os.path.exists(filename):
                    try:
                        os.makedirs(os.path.dirname(filename))
                    except:
                        pass

                # Write add peer index to file
                with open(filename, 'w') as out:
                    out.write(self.mrt_encode_dump_peer_index_msg(dump_timestamp,
                                                                  [{'ip_address': str(peer[2]),
                                                                    'bgp_id': str(peer[5]),
                                                                    'asn': str(peer[6])}]))

                #
                # QUERY for RIB ENTRIES
                #
                # self.thread_query("select prefix,prefix_len,floor(unix_timestamp(rib.timestamp)),"
                #                   "        a.origin,a.as_Path,a.next_hop,a.med,a.local_pref,"
                #                   "        a.isAtomicAgg,a.aggregator,a.community_list,"
                #                   "        a.originator_id,a.cluster_list "
                #                   "    from rib join path_attrs a on (rib.path_attr_hash_id = a.hash_id)"
                #                   "    where rib.peer_hash_id = '%s' and isWithdrawn = False"
                #                   % peer[4], filename, dump_timestamp)
                self._db_conn.query_cb("select prefix,prefix_len,floor(unix_timestamp(rib.timestamp)),"
                                      "        a.origin,a.as_Path,a.next_hop,a.med,a.local_pref,"
                                      "        a.isAtomicAgg,a.aggregator,a.community_list,"
                                      "        a.originator_id,a.cluster_list "
                                      "    from rib join path_attrs a on (rib.path_attr_hash_id = a.hash_id)"
                                      "    where rib.peer_hash_id = '%s' and isWithdrawn = False"
                                      % peer[4],None,
                                      self.cb_write_mrt_rib_entries, filename, dump_timestamp)

            # Wait for remaining threads
            self.check_threads(True)

            self.LOG.info("Completed dumps")

        else:
            self.LOG.error("Table dump couldn't be performed due to a connection problem to MySQL. Will try next time")

        # Close the DB connection
        self._db_conn.close()

    def cb_write_mrt_rib_entries(self, rows, sequence_start, filename=None, dump_timestamp=None, peer_index=0):
        """ Writes the MRT table_dump_v2 RIB entries based on the supplied rows/query results

            :param rows:            Query results, list has a specific order
            :param sequence_start:  Starting number for the sequence
            :param filename:        Filename of table_dump output file - contents will be appended
            :param dump_timestamp:  timestamp in seconds used for the log filename
            :param peer_index:      Peer Index for rib entry

        """
        if not filename or not rows:
            self.LOG.error("No rib entries to write or filename is not defined")
            return

        with open(filename, 'a') as out:
            seq = sequence_start + 1
            for rib_entry in rows:
                out.write(self.mrt_encode_dump_rib_unicast_msg(dump_timestamp, seq, rib_entry, peer_index))
                seq += 1

        self.LOG.debug("%s: rib entries %d processed", filename, len(rows))

    def thread_query(self, query, filename,dump_timestamp):
        """ Threads the query.  If max threads has been reached, this will
            block till one is available.

            :param query:    The SQL query to run
            :param filename: Filename for the results to be written to
            :param dump_timestamp:      timestamp in seconds used for the log filename
        """
        started = False

        # block till the thread is started
        while not started:
            self.check_threads()

            # Loop through available threads and try to start new ones to process the peer
            for idx in range(0, self.MAX_THREADS):
                if idx not in self._thr_list:
                    self._thr_list[idx] = {'thr': None, 'running': False }

                if not self._thr_list[idx]['running']:
                    self.LOG.debug("running thread %d", idx)

                    # Run thread for query
                    self._thr_list[idx]['thr'] = threading.Thread(
                                  target=self._db_conn[idx].query_cb,
                                  args=(query, None,
                                        self.cb_write_mrt_rib_entries,
                                        filename, dump_timestamp))

                    self._thr_list[idx]['thr'].start()
                    self._thr_list[idx]['running'] = True
                    started = True
                    break

            if not started:
                self.LOG.debug("Reached max threads, blocking and trying again...")
                time.sleep(0.500)

    def check_threads(self, waitForAll=False):
        """ Joins the running threads to clean them up

            :param waitForAll:      True join and wait/block till all threads are complete

            :return: Count of threads still running after attempting to join
        """
        count = 0
        for idx in self._thr_list:
            if self._thr_list[idx]['running']:
                if not waitForAll:
                    self._thr_list[idx]['thr'].join(0.1)
                else:
                    self._thr_list[idx]['thr'].join()

                # Check if the thread is done after the join
                if not self._thr_list[idx]['thr'].isAlive():
                    self._thr_list[idx]['thr'] = None
                    self._thr_list[idx]['running'] = False

                else:
                    count += 1

        self.LOG.debug("Threads running after check (%r) is %d", waitForAll, count)
        return count

    def mrt_encode_common_hdr(self, timestamp, subtype, data_len):
        """ Encode a MRT common header

            :param timestamp:       Timestamp used for the message
            :param subtype:         Table dump v2 subtype
            :param data_len:        Data length (message length)

            :return: MRT data ready to be written
        """
        # 0                   1                   2                   3
        # 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                           Timestamp                           |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |             Type              |            Subtype            |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                             Length                            |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                      Message... (variable)
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        mrt_msg = pack("!IHHI", timestamp,
                       TYPE_TABLE_DUMP_V2,
                       int(subtype),
                       data_len)
        return mrt_msg

    def mrt_encode_dump_peer_index_msg(self, timestamp, peer_list):
        """ Encode MRT table_dump_v2 subtype PEER_INDEX_TABLE

            :param timestamp:    Timestamp used for the message
            :param peer_list:    List of dictionaries in the format::
                    {
                      ip_address: <ip_address>,
                      bgp_id: <ip address>,
                      asn: <asn for the peer>
                    }
            :return: MRT data ready to be written
        """
        # 0                   1                   2                   3
        # 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                      Collector BGP ID                         |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |        View Name Length        |     View Name (variable)      |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |          Peer Count           |    Peer Entries (variable)
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        data_msg = socket.inet_pton(socket.AF_INET, self._cfg['table_dump']['collector_bgp_id'])
        data_msg += pack("!HH", 0, len(peer_list))

        for peer in peer_list:
            # 0                   1                   2                   3
            # 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
            # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            # |   Peer Type   |
            # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            # |                         Peer BGP ID                           |
            # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            # |                   Peer IP Address (variable)                  |
            # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            # |                        Peer AS (variable)                     |
            # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            if not ':' in peer['ip_address']:
                data_msg += pack('!B', 2)  # 4-octet ASN, IPv4 address
                afi = socket.AF_INET
            else:
                data_msg += pack('!B', 3)  # 4-octet ASN, IPv6 address
                afi = socket.AF_INET6

            data_msg += socket.inet_pton(socket.AF_INET, peer['bgp_id'])

            data_msg += socket.inet_pton(afi, peer['ip_address'])
            data_msg += pack("!I", long(peer['asn']))

        # Generate common header and add data/message
        mrt_hdr = self.mrt_encode_common_hdr(timestamp,
                                             SUBTYPE_DUMP_V2_PEER_INDEX, len(data_msg))
        return mrt_hdr + data_msg

    def mrt_encode_dump_rib_unicast_msg(self, timestamp, sequence, rib_entry, peer_index):
        """ Encode MRT table_dump_v2 subtype RIB_IPV4_UNICAST or RIB_IPV6_UNICAST

            :param timestamp:    Timestamp used for the message
            :param sequence:     Sequence number
            :param rib_entry:    Rib entry from SQL query response
            :param peer_index:   Peer index for RIB entry

            :return: MRT data ready to be written
        """
        if ':' not in str(rib_entry[0]):
            afi = socket.AF_INET
            subtype = SUBTYPE_DUMP_V2_RIB_IPV4_UNICAST
        else:
            afi = socket.AF_INET6
            subtype = SUBTYPE_DUMP_V2_RIB_IPV6_UNICAST

        # 0                   1                   2                   3
        # 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                         Sequence Number                       |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # | Prefix Length |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                        Prefix (variable)                      |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |         Entry Count           |  RIB Entries (variable)
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        data_msg = pack("!IB", sequence, int(rib_entry[1]))

        # Calculate the number of bytes needed to encode the prefix
        prefix_bytes = int(rib_entry[1]) / 8
        if int(rib_entry[1]) % 8 > 0:
            prefix_bytes += 1

        data_msg += socket.inet_pton(afi, str(rib_entry[0]))[:prefix_bytes]

        data_msg += pack("!H", 1)           # Only one entry since this is by peer

        # RIB entry
        #  0                   1                   2                   3
        #  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |         Peer Index            |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                         Originated Time                       |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |      Attribute Length         |
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        # |                    BGP Attributes... (variable)
        # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        data_msg += pack("!HI", peer_index, long(rib_entry[2]))

        # build attributes data
        attrs = bgp.encode_attr_origin(str(rib_entry[3]))
        attrs += bgp.encode_attr_as_path(str(rib_entry[4]))

        if afi == socket.AF_INET:
            attrs += bgp.encode_attr_nexthop(str(rib_entry[5]))
        else:
            attrs += bgp.encode_attr_mp_nexthop(str(rib_entry[5]))

        if long(rib_entry[6]) > 0:
            attrs += bgp.encode_attr_med(long(rib_entry[6]))

        if long(rib_entry[7]) > 0:
            attrs += bgp.encode_attr_local_pref(long(rib_entry[7]))

        if int(rib_entry[8]) == 1:
            attrs += bgp.encode_attr_atomic_agg()

        if len(str(rib_entry[9])) > 0:
            attrs += bgp.encode_attr_aggregator(str(rib_entry[9]))

        if len(str(rib_entry[10])) > 0:
            attrs += bgp.encode_attr_communities(str(rib_entry[10]))

        if len(str(rib_entry[11])) > 0:
            attrs += bgp.encode_attr_originator_id(str(rib_entry[11]))

        if len(str(rib_entry[12])) > 0:
            attrs += bgp.encode_attr_cluster_list(str(rib_entry[12]))

        # Add attributes to data message
        data_msg += pack('!H', len(attrs))
        data_msg += attrs

        # Generate common header and add data/message
        mrt_hdr = self.mrt_encode_common_hdr(timestamp, subtype, len(data_msg))

        return mrt_hdr + data_msg

    def stop(self):
        self._stop.set()

    def stopped(self):
        return self._stop.is_set()
